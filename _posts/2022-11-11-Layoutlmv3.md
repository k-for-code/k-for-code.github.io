
# **LAYOUTLMV3**


## 
Pre-training for Document AI with Unified Text and Image Masking




![](/images/image1.png)





# 
**L(0) View**


## 
**Input**


Inputs - text, bbox, image


The input of Transformer is a concatenation of text embedding and image embedding sequences


## 
**Output**


 text-and-image contextual representations.


## 
**Pretraining**


LayoutLMv3 is pre-trained with the MLM, MIM, and WPA


## 
**Model Configurations**


Base - 12 layers


Large - 24 layers


# 
**L(1) View**


## 
**Input**


### 
**Text Embedding**
![](/images/image2.png)


* Text embedding is a combination of word embeddings and position embeddings.
* Used OCR toolkit to obtain textual content and corresponding 2D position information (bbox).
* Bbox are of normalized image  —** find size  (normalized image size 224*224)**
*  they initialize the word embeddings with a word embedding matrix from a pre-trained model RoBERTa.  — the **first layer **
* The position embeddings include 1D position and 2D layout position embeddings, where the 1D position refers to the index of tokens within the text sequence,  **and the 2D layout position refers to the bounding box coordinates of the text**
* Unlike v1 and v2 where each word has its bbox,** in v3 they adopt segment-level layout positions that words in a segment share the same 2D position since the words usually express the same semantic meaning**
    * Paper reference 1. StructuralLM: Structural Pre-training for Form Understanding
* The position embedding for each token changes but the 2d embeddings remain same – for same block of tokens  – **refer below image**

### 
**Image Embedding**

* Unlike v1,v2 and other architectures which rely on CNN and Object detection models for image features they
* **represent document images with linear projection features of image patches** before feeding them into the multimodal Transformer.**_<span style="text-decoration:underline;"> (find out how)</span>_**
* resize a document image into 𝐻 ×𝑊 and denote the image with I ∈ R 𝐶×𝐻×𝑊 , where 𝐶, 𝐻 and𝑊 are the channel size, width and height of the image respectively.
*  they then split the image into a sequence of uniform 𝑃 × 𝑃 patches, linearly project the image patches to 𝐷 dimensions and flatten them into a sequence of vectors, which length is 𝑀 = 𝐻*𝑊 /𝑃*P . **_<span style="text-decoration:underline;"> (find value of P,D )</span>_** 
*  𝑃 = 16, 𝑀 = 196
* Add learnable 1D position embeddings to each patch

## 
**Output**


 text-and-image contextual representations.


## 
**Pretraining**


### 
**MVLM**

* mask 30% of text tokens with a span masking strategy with span lengths drawn from a Poisson distribution (𝜆 = 3)  –** similar to ROberta** (but static masking)


### **Masked Image Modeling (MIM)**



* mask  40% image tokens with the**_ blockwise (patch) masking strategy_** - **similar to BEit**
* Uses cross-entropy loss to** reconstruct the masked image tokens under the context of their surrounding text and image tokens**
* MIM facilitates learning high-level layout structures rather than noisy low-level details
* 


### **Word-Patch Alignment (WPA)**



* WPA objective is to predict whether the corresponding image patches of a text word are masked ( masked in MLM and MIM)
* they assign an aligned label to an unmasked text token when its corresponding image tokens are also unmasked. Otherwise, they assign an unaligned label
* they exclude the masked text tokens when calculating WPA loss to prevent the model from learning a correspondence between masked text words and image patches
* they use a two-layer MLP head that inputs contextual text and image and outputs the binary aligned/unaligned labels with a binary cross-entropy loss

## 
** Setup**

*  pre-train LayoutLMv3 on a large IIT-CDIP dataset. 
* The IITCDIP Test Collection 1.0 is a large-scale scanned document image dataset, which contains about 11 million document images and can be split into** 42 million pages **
* used 11 million of them to train LayoutLMv3. 
* they do not do image augmentation following LayoutLM models. For the multimodal Transformer encoder along with the text embedding layer,
*  **LayoutLMv3 is initialized from the pre-trained weights of RoBERTa**.
* ** image tokenizer is initialized from a pre-trained image tokenizer in DiT**, a self-supervised pre-trained document image Transformer model. 
* The vocabulary size of image tokens is 8,192.
*  They randomly initialized the rest model parameters.
*  they pre-train LayoutLMv3 using
    *  Adam optimizer
    * batch size of 2,048 for 500,000 steps.
    * weight decay of 1𝑒 − 2, and (𝛽1, 𝛽2) = (0.9, 0.98). 
    * LayoutLMv3BASE model, learning rate of 1𝑒 −4, 
    * linearly warm up the learning rate over the first 4.8% steps
    * LayoutLMv3LARGE, the learning rate and warm-up ratio are 5𝑒 − 5 and 10%
    * 32 NVIDIA Tesla V100 GPUs with 32GB memory

## 
**Model Configurations**


**Base - 12 layers**


12-head self-attention, hidden size of 𝐷 = 768, and 3,072 intermediate size of feed-forward networks.


**Large - 24 layers**


 16-head self-attention, hidden size of 𝐷 = 1, 024, and 4,096 intermediate size of feed-forward networks

* To pre-process the text input, they tokenize the text sequence with Byte-Pair Encoding (BPE) [46] with a maximum sequence length 𝐿 = 512.
*  they add a [CLS] and a [SEP] token at the beginning and end of each text sequence. When the length of the text sequence is shorter than 𝐿, they append [PAD] tokens to it.
*  The bounding box coordinates of these special tokens are all zeros.
*  The parameters for image embedding are 𝐶 × 𝐻 ×𝑊 = 3 × 224 × 224, 𝑃 = 16, 𝑀 = 196
* They adopt distributed and mixed-precision training to reduce memory costs and speed up training procedures. They have also used a gradient accumulation mechanism to split the batch of samples into several mini-batches to overcome memory constraints for large batch sizes. They further use a gradient checkpointing technique for document layout analysis to reduce memory costs. To stabilize training, they follow CogView [10] to change the computation of attention to softmax  Q𝑇 K √ 𝑑  = softmax   Q𝑇 𝛼 √ 𝑑 K − max  Q𝑇 𝛼 √ 𝑑 K   × 𝛼  , where 𝛼 is 32.

# 
**L(2) View**


## 
**Input**


### 
**Text Embeddings**

* Num of tokens 512 
* Word embedding - (50265, 768)
* Roberta word embeddings 
* Token_type_embeddings – torch.zeros()




![alt_text](/images/image7.png "image_tooltip")

![alt_text](/images/image6.png "image_tooltip")



### **Image Embeddings**


![alt_text](/images/image4.png "image_tooltip")



* BEIT uses P=14 and gets 16 dimensional patches, but** V3 uses p=16 and gets 14 dimensional 197 patches**
* Then the used the below mentioned COnv2d layer to project it to 196 dimensions using 768 filters, refer below calculations
* sqrt(224*224/(16*16)) = 14
    * Patch_size = 16,16
    * Patch_shape = 14,14
    * `self.patch_shape = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])`
* `Projection = nn.Conv2d(config.num_channels, config.hidden_size, kernel_size=patch_size, stride=patch_size)`
    * `Pixel_values = [2, 3, 224, 224])`
    * `conv2d(3,768,16,16)`
    * `embeddings = self.proj(pixel_values)`
    * `Embeddings - torch.Size([2, 768, 14, 14])`
    * `embeddings = embeddings.flatten(2).transpose(1, 2)`
    * `Embeddings - batch_size, 196,768`
* `Finally they concat the text and visual embeddings`
    * `embedding_output = torch.cat([embedding_output, visual_embeddings], dim=1)`
    * `Emebedding_output.shape == [batch_size,709,768]`


## **Output**



* Output size (batch_size, 709, 768)
* Text token + visual cls tok + visual token
* 512 + 1 + 196 = 709


## **Ablation Study**




![alt_text](/images/image3.png "image_tooltip")




* they first built a baseline model #1 that uses text and layout information, pre-trained with MLM objective. 
* Then they use linearly projected image patches as the image embedding of the baseline model, denoted as model #2
* Then further pretrain with MIM and WPA


### **Effect of Linear Image Embedding. **



* Without image embedding has achieved good results on some tasks
*  model #1 cannot conduct some image-centric document analysis tasks without vision modality. For example, the vision modality is critical for the document layout analysis task on PubLayNet because bounding boxes are tightly integrated with images. 
* simple design of linear image embedding combined with appropriate pre-training objectives can consistently improve not only image-centric tasks, but also some text-centric tasks further


### **Effect of MIM pre-training objective. **



* Simply concatenating linear image embedding with text embedding as input to model #2 deteriorates performance on CORD (IE) and RVL-CDIP (Img Doc class), while the loss on PubLayNet diverges. We speculate that the model failed to learn meaningful visual representation on the linear patch embeddings without any pre-training objective associated with image modality. The MIM objective mitigates this problem by preserving the image information until the last layer of the model by randomly masking out a portion of input image patches and reconstructing them in the output [22]. Comparing the results of model #3 and model #2, the MIM objective benefits CORD and RVL-CDIP.** As simply using linear image embedding has improved FUNSD, MIM does not further contribute to FUNSD.** By incorporating the MIM objective in training, the loss converges when fine-tuning PubLayNet as shown in Figure 4, and we obtain a desirable mAP score. The results indicate that MIM can help regularize the training. Thus MIM is critical for vision tasks like document layout analysis on PubLayNet.


### **Effect of WPA pre-training objective.**



*  WPA objective consistently improves all tasks. Moreover, the WPA objective decreases the loss of the vision task on PubLayNet in Figure 4. These results confirm the effectiveness of WPA not only in cross-modal representation learning, but also in image representation learning.


### **Finetuning on FUNSD**



* Note that LayoutLMv3 and StructuralLM use segment-level layout positions, while the other works use word level layout positions. Using segment-level positions may benefit the semantic entity labeling task on FUNSD Check out appendix for more info on this

## 
**Model Configuration**


## 
**Attention Mechanism**

* Considering the large range of positions, they model the semantic relative position and spatial relative position as bias terms to prevent adding too many parameter — **They calculate realtive postion and ass it to the attention score**
* **_<span style="text-decoration:underline;">attention_scores += rel_pos / math.sqrt(self.attention_head_size)</span>_**
* To calculate the 2d relative position they subtract x1 and y2 of each bboxes
* To calculate 1d relative positions they subtract each position id with other one, 
* The above step result in a matrix, on which they apply binning(group nearby numbers into a fixed category)
* Then they one-hot encode each bin and pass them through a linear layer, 
* The output of above step is added to attention scores


```
Code snippets for reference 
batch_size, seq_len, _ = embeddings.size()
       cls_tokens = self.cls_token.expand(batch_size, -1, -1)
       embeddings = torch.cat((cls_tokens, embeddings), dim=1)
if config.visual_embed:
           # use the default pre-training parameters for fine-tuning (e.g., input_size)
           # when the input_size is larger in fine-tuning, they will interpolate the position embeddings in forward
           self.patch_embed = LayoutLMv3PatchEmbeddings(config)

           size = int(config.input_size / config.patch_size)
           self.cls_token = nn.Parameter(
               torch.zeros(1, 1, config.hidden_size))
           self.pos_embed = nn.Parameter(torch.zeros(
               1, size * size + 1, config.hidden_size))
           self.pos_drop = nn.Dropout(p=0.0)

Config.input_size = 224
Config.patch_size = 16
Szie = 14
Pos_embed = (1, 14*14 + 1, 768)  —-> (1,197, 768)
```







![alt_text](images/image7.png "image_tooltip")



```
embedding_output = torch.cat([embedding_output, visual_embeddings], dim=1)
Emebedding_output.shape == [2,709,768]
```





![alt_text](images/image8.png "image_tooltip")







![alt_text](images/image9.png "image_tooltip")







![alt_text](images/image10.png "image_tooltip")



```
if self.config.has_relative_attention_bias or self.config.has_spatial_attention_bias:
               if self.config.has_spatial_attention_bias:
                   visual_bbox = self.calculate_visual_bbox(
                       device, dtype=torch.long, batch_size=batch_size)
                   if bbox is not None:
                       final_bbox = torch.cat([bbox, visual_bbox], dim=1)
                   else:
                       final_bbox = visual_bbox
```







![alt_text](images/image11.png "image_tooltip")



```
def calculate_visual_bbox(self, device, dtype, batch_size):
       visual_bbox = self.visual_bbox.repeat(batch_size, 1, 1)
       visual_bbox = visual_bbox.to(device).type(dtype)
       return visual_bbox
```


Position_ids start from 2–513    1 is used for padding indexes

**# Code to see relative position on bboxes**


```
import math
import torch
from torch import nn
import torch.nn.functional as F

cnfg = {
   "rel_pos_bins": 32,
   "max_rel_pos": 128,
   "rel_2d_pos_bins": 64,
   "max_rel_2d_pos": 256,
   "hidden_size": 768,
   "num_hidden_layers": 12,
   "num_attention_heads": 12
}

config = Dict2Class(cnfg)

def init_visual_bbox(image_size=(14, 14), max_len=1000):
   """
   Create the bounding boxes for the visual (patch) tokens.
   """
   visual_bbox_x = torch.div(
       torch.arange(0, max_len * (image_size[1] + 1), max_len), image_size[1], rounding_mode="trunc"
   )

   print(visual_bbox_x)
   visual_bbox_y = torch.div(
       torch.arange(0, max_len * (image_size[0] + 1), max_len), image_size[0], rounding_mode="trunc"
   )
   visual_bbox = torch.stack(
       [
           visual_bbox_x[:-1].repeat(image_size[0], 1),
           visual_bbox_y[:-1].repeat(image_size[1], 1).transpose(0, 1),
           visual_bbox_x[1:].repeat(image_size[0], 1),
           visual_bbox_y[1:].repeat(image_size[1], 1).transpose(0, 1),
       ],
       dim=-1,
   ).view(-1, 4)

   cls_token_box = torch.tensor([[0 + 1, 0 + 1, max_len - 1, max_len - 1]])
   visual_bbox = torch.cat([cls_token_box, visual_bbox], dim=0)
   return visual_bbox

def calculate_visual_bbox(batch_size=2):
   visual_bbox = init_visual_bbox()
   visual_bbox = visual_bbox.repeat(batch_size, 1, 1)
   return visual_bbox

def relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):
   ret = 0
   if bidirectional:
       num_buckets //= 2
       ret += (relative_position > 0).long() * num_buckets
       n = torch.abs(relative_position)
   else:
       n = torch.max(-relative_position, torch.zeros_like(relative_position))
   # now n is in the range [0, inf)

   # half of the buckets are for exact increments in positions
   max_exact = num_buckets // 2
   is_small = n < max_exact

   # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance
   val_if_large = max_exact + (
       torch.log(n.float() / max_exact) / math.log(max_distance /
                                                   max_exact) * (num_buckets - max_exact)
   ).to(torch.long)
   val_if_large = torch.min(
       val_if_large, torch.full_like(val_if_large, num_buckets - 1))

   ret += torch.where(is_small, n, val_if_large)
   return ret

def _cal_1d_pos_emb(position_ids):

   rel_pos_bins = config.rel_pos_bins
   max_rel_pos = config.max_rel_pos
   rel_pos_onehot_size = config.rel_pos_bins
   rel_pos_bias = nn.Linear(
       rel_pos_onehot_size, config.num_attention_heads, bias=False)

   rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)

   rel_pos = relative_position_bucket(
       rel_pos_mat,
       num_buckets=rel_pos_bins,
       max_distance=max_rel_pos,
   )
   rel_pos = F.one_hot(
       rel_pos, num_classes=rel_pos_onehot_size).type(torch.FloatTensor)
   rel_pos = rel_pos_bias(rel_pos).permute(0, 3, 1, 2)
   rel_pos = rel_pos.contiguous()
   return rel_pos

def _cal_2d_pos_emb(bbox):
   max_rel_2d_pos = config.max_rel_2d_pos
   rel_2d_pos_bins = config.rel_2d_pos_bins
   rel_2d_pos_onehot_size = config.rel_2d_pos_bins
   rel_pos_x_bias = nn.Linear(
       rel_2d_pos_onehot_size, config.num_attention_heads, bias=False)
   rel_pos_y_bias = nn.Linear(
       rel_2d_pos_onehot_size, config.num_attention_heads, bias=False)

   position_coord_x = bbox[:, :, 0]
   position_coord_y = bbox[:, :, 3]
   rel_pos_x_2d_mat = position_coord_x.unsqueeze(
       -2) - position_coord_x.unsqueeze(-1)
   rel_pos_y_2d_mat = position_coord_y.unsqueeze(
       -2) - position_coord_y.unsqueeze(-1)

   rel_pos_x = relative_position_bucket(
       rel_pos_x_2d_mat,
       num_buckets=rel_2d_pos_bins,
       max_distance=max_rel_2d_pos,
   )
   rel_pos_y = relative_position_bucket(
       rel_pos_y_2d_mat,
       num_buckets=rel_2d_pos_bins,
       max_distance=max_rel_2d_pos,
   )
   rel_pos_x = F.one_hot(
       rel_pos_x, num_classes=rel_2d_pos_onehot_size).type(torch.FloatTensor)
   rel_pos_y = F.one_hot(
       rel_pos_y, num_classes=rel_2d_pos_onehot_size).type(torch.FloatTensor)
   rel_pos_x = rel_pos_x_bias(rel_pos_x).permute(0, 3, 1, 2)
   rel_pos_y = rel_pos_y_bias(rel_pos_y).permute(0, 3, 1, 2)
   rel_pos_x = rel_pos_x.contiguous()
   rel_pos_y = rel_pos_y.contiguous()
   rel_2d_pos = rel_pos_x + rel_pos_y
   return rel_2d_pos
```



# **Appendix**


Useful excerpts from structuralLM paper


**Cell Position Classification**.


 In addition to the MVLM, we propose a new Cell Position Classification (CPC) task to model the relative spatial position of cells in a document. Given a set of scanned documents, this task aims to predict where the cells are in the documents. First, we split them into N areas of the same size. Then we calculate the area to which the cell belongs to through the center 2D-position of the cell. Meanwhile, some cells are randomly selected, and the 2D-positions of tokens in the selected cells are replaced with (0; 0; 0; 0). During the pre-training, a classification layer is built above the encoder outputs. This layer predicts a label [1, N] of the area where the selected cell is located, and computes the cross-entropy loss. Considering the MVLM and CPC are performed simultaneously, the cells with masked tokens will not be selected for the CPC task. This prevents the model from not utilizing cell-level layout information when doing the MVLM task. We will compare the performance of different N in Section 3.1






![alt_text](/images/image8.png "image_tooltip")



First, we evaluate how much the cell-level layout embedding contributes to form understanding by removing it from StructuralLM pre-training. This ablation results in a drop from 0.8514 to 0.8024 on F1 score, demonstrating the important role of the cell-level layout embedding. To study the effect of the cell position classification task in StructuralLM, we ablate it and the F1 score significantly drops from 0.8514 to 0.8125. Finally we study the significance of full StructuralLM pretraining. Over 15% of performance degradation resulted from ablating pre-training clearly demonstrates the power of StructuralLM in leveraging an unlabeled corpus for downstream form understanding tasks. Actually, after ablating the cell position classification, the biggest difference between StructuralLM and LayoutLM is cell-level 2D-position embeddings or word-level 2D-position embeddings. The results show that StructuralLM with cell-level 2D-position embeddings performs better than LayoutLM with word-level position embeddings with an improvement of over 2% F1-score point (from 0.7895 to 0.8125). Furthermore, we compare the performance of the MVLM with cell-level layout embeddings and word-level layout embeddings respectively. As shown in Figure 5, the results show that under the same pre-training settings, the MVLM training loss with cell-level 2D-position embeddings can converge lower


# **Summary**



* Block/ cell level 2d positions help > word level
* Use of relative position in self attention helps
* Image doesn’t help much on FUNSD ( any text centric task )
* Pretraining using MLM to understand language is very important
* Weight initialisation using ROberta >> BERT >>> scratch
* 
