---
title: " Compressing Transformer Based Models"
categories:
  - MLops
classes: wide
excerpt: Literature Review of Compressing BERT.
header:
  og_image: /images/bert_components.png
  teaser: "/images/layers_mem_compute_analysis.png"
---




# Breakdown and Analysis


    BERT-BASE model by running inference on a sentence of length 256



![layers_mem_compute_analysis](/images/layers_mem_compute_analysis.png)

* The most memory and computation heavy unit in a transformer is FFN
* Embedding layers huge part of model params, but less computation ( just lookup)
* Self-Attention has no learnable params but is computation heavy during runtime 


# Compression Methods


## Knowledge Distillation



* Knowledge Distillation refers to training a smaller model (called the _student_) using outputs (from various intermediate functional components) of one or more larger pre-trained models (called the _teachers_).


![alt_text](/images/kd.png "image_tooltip")

* reducing the encoder width
    * challenge - student cannot directly learn from the teacher’s intermediate outputs, due to different sizes. To overcome - student down-projecting/ Up-projecting to the original dimension
* reducing the number of encoders
    * preserving the bottom encoder units and aggressively distilling the top encoder units yields a better-performing student model
* replacing with a BiLSTM , replacing with a CNN, or some combination thereof.
    * only the output from the last encoder unit can be used for distillation
* 

![alt_text](/images/distil_vs_tiny.png "image_tooltip")

* Minilm - tinybert + value to value loss
* BERT of Thesus -freeze the teacher modules and train a hybrid model by randomly replacing some modules in the teacher model with student modules.
* [https://www.linkedin.com/pulse/best-practices-text-classification-distillation-part-14-wasserblat](https://www.linkedin.com/pulse/best-practices-text-classification-distillation-part-14-wasserblat)


## Pruning



* Pruning refers to identifying and removing redundant or less important weights and/or components, which sometimes even makes the model more robust and better-performing


###  Unstructured Pruning



* prunes individual weights by locating the set of the least important weights in the model judged by their absolute values, by the gradients


### Structured Pruning.



* Prunes specific modules
* Attention pruning, Encoder unit pruning, Embeddings pruning


#### Attention Head Pruning



* self-attention layer incurs considerable computational overhead at inference
* high accuracy is possible with only 1–2 attention heads per encoder unit
    * [https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html) 


## Quantization



* Quantization refers to reducing the number of unique values required to represent model weights and activations, which allows to represent them using fewer bits
* Specialized hardwares present to optimize runtime memory consumption
* Naive approach - truncate every weight to target. Results poor - drift in weights called quantization noise
* Assume normal distribution only quantize only in 1 standard deviation keep the outliers
* Cluster the weights – replace them with their centroids
* post-training quantization (PTQ) and quantization-aware training (QAT). PTQ rescales the weights of a trained model whereas QAT introduces the rounding error into the training process. Due to the considerable performance drop for PTQ, most works in compressing NLP models unanimously use QAT,
* Training with Quantization Noise for Extreme Model Compression  [https://arxiv.org/abs/2004.07320](https://arxiv.org/abs/2004.07320)
    * **_<span style="text-decoration:underline;"> 82.5% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.</span>_**
    * QAT + Straight-Through Estimator (STE)
* **[https://github.com/intel/neural-compressor/tree/master/neural_coder](https://github.com/intel/neural-compressor/tree/master/neural_coder)**


## Encoder-Decoder Sharing



* Tied Transformer

![alt_text](/images/tied_transf.png "image_tooltip")


## Layer Sharing

* Like ALBERTA - reuse encoder block
* [ALBERTA](https://k-for-code.github.io/Alberta/)


![alt_text](/images/albert-parameter-sharing.png "image_tooltip")

* “sandwich-style” parameter sharing, which shares the weights for central layers while leaving the first and last layers independent


## Matrix Decomposition


### Embedding Decomposition


![alt_text](/images/embedding-decompose-albert.png "image_tooltip")



### Attention Decomposition

[https://arxiv.org/abs/2005.00697](https://arxiv.org/abs/2005.00697) 


![alt_text](/images/attn_decompose.png "image_tooltip")



## Early Exit and Progressive Word Vector Elimination



* separate classifiers can be trained for each encoder unit output. 
* Training these separate classifiers can be done either from scratch or by distilling the output of the final classifier
* [https://arxiv.org/abs/2004.02178](https://arxiv.org/abs/2004.02178) - FastBERT
* Exit criterion
    * max class probability > θ
    * entropy &lt; θ 
* Progressive elimination
    * Used in classifiers, since CLS used, keep eliminating words
    * Different methods used to eliminate word
    * [https://proceedings.mlr.press/v119/goyal20a.html](https://proceedings.mlr.press/v119/goyal20a.html) - PowerBERT




![alt_text](/images/early_exit.png "image_tooltip")



# What Works



* Quantization and unstructured pruning can provide tremendous boost in terms of speed with negligible loss in performance when executed on specialized hardware or with specialized processing libraries.
* KD like tiny bert - with attention
* applications with strict latency constraints, replacing Transformers with alternative units is a better choice
* [https://dl.acm.org/doi/abs/10.1145/3466752.3480095](https://dl.acm.org/doi/abs/10.1145/3466752.3480095) - EdgeBERT Shows combination of multiple strategies
* Quantize, Prune then Distil [https://arxiv.org/abs/2109.14960](https://arxiv.org/abs/2109.14960) 

References



* [https://ieeexplore.ieee.org/document/8253600](https://ieeexplore.ieee.org/document/8253600) 
* [https://www.ijcai.org/proceedings/2020/341](https://www.ijcai.org/proceedings/2020/341) 
* [https://ieeexplore.ieee.org/document/9194522](https://ieeexplore.ieee.org/document/9194522) 
* [https://arxiv.org/abs/2202.07105](https://arxiv.org/abs/2202.07105) 
* [https://doi.org/10.1162/tacl_a_00413]([https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A](https://doi.org/10.1162/tacl_a_00413)) 
